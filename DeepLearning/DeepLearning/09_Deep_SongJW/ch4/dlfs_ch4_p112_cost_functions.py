import numpy as np

'''
4.2 손실함수
비용함수 cost function or 손실함수 loss functions
 신경망에서 학습이 제대로 이루어졌는지 평가하는 하나의 지표.
 비용함수의 값을 최소로 하는 가중치를 찾는것이 학습의 목표.
    1. 평균제곱오차 mean squared error - MSE
    2. 교차 엔트로피 오차 cross entropy error - CEE 
'''

t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # 답 : 2
y1 = np.array([.1, .05, .6, .0, .05, .1, .0, .1, .0, .0])    # 예측 : 2 (확률 0.6)
y2 = np.array([.1, .05, .1, .0, .05, .1, .0, .6, .0, .0])    # 예측 : 7 (확률 0.6)

#####
# 4.2.1 평균 제곱 오차 MSE
# E = (1/2) * (sum( (y - e)**2 ))
#   (y = 신경망의 예측값, t = 실제라벨, y, t 모두 one-hot-encoding)
#
# ex) mnist
# y = [ 0.1,    0.05,   0.6,    0.0,    0.05,   0.1,    0.0,    0.1,    0.0,    0.0 ]
# t = [ 0,      0,      1,      0,      0,      0,      0,      0,      0,      0   ]
# y 는 신경망이 예측한 입력데이터가 0~9 까지 각 숫자일 확률
# t 는 실제 입력데이터의 라벨
# MSE 는 대응하는 y, t 값의 차를 제곱하여 모두 더한 후 2로 나눈 값
#####

def MeanSquaredError(y, t):
    return np.sum((y - t) ** 2)/2

print(MeanSquaredError(y1, t))   # 정답 예측 - 0.0975
print(MeanSquaredError(y2, t))   # 오답 예측 - 0.5975

    # 잘못된 예측을 하는 경우 MSE 값이 커지게된다.
    # 정확한 값을 예측할뿐만 아니라 더 높은 확률을 부여할수록 오차는 작아지게된다.


#####
# 4.2.2 교차 엔트로피 오차 CEE
# E = - sum(t * log(y))
# log 는 자연로그 ln, y 와 t 모두 one-hot-encoding
# 정답인 레이블만 제외하면 t 의 각 요소는 모두 0이 된다.
# 따라서 하나의 입력 데이터에 대해서 E = -log(예측확률), 정답일때의 출력이 전체 값을 정하게 된다.
# 로그 함수는 입력이 0에 가까울수록 급격하게 증가하므로 잘못된 예측을 하거나
# 예측이 맞아도 예측확률이 낮다면 급격하게 에러가 증가한다.
# 이상적이게 예측이 완벽하다면 ( 확률 : 1.0 ) 에러는 0이 된다.
#####

def CrossEntropyError(y, t):
    delta = 1e-7    # 로그함수에는 0 이 입력될 수 없음. y 에 0 값이 존재할 수 있으니 작은 값을 넣어줌.
    return -np.sum(t * np.log(y + delta))

print(CrossEntropyError(y1, t))
    # 정답 예측 - 0.510825457099
print(CrossEntropyError(y2, t))
    # 오답 예측 - 2.30258409299
    # 이 값은 오답에 높은 확률을 부여했기때문이 아닌 정답에 낮은 확률을 부여했기 때문에 계산되는 오차.

