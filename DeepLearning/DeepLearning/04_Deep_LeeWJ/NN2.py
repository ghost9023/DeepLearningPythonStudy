'''

딥러닝이란 것은 심층 신경망(Deep Neural Network)을
이용하여 지도학습, 비 지도학습, 강화학습을 수행해서 데이터를 분류, 분석,
예측하는 것을 의미합니다.

뉴런의 입력은 다수이나, 출력은 하나이며, 여러 신경세포로부터 온
신호들이 합산되어 출력되는 것이다.
그 합산된 값은 출력층에서의 활성화함수에 들어가서
임계치를 기준으로 출력신호가 생길수도, 생기지 못할수도 있는 구조를 가진다.

이에 은닉층이라는 개념이 생긴다.

입력을 받는 입력층
학습이나 분류하는 은닉층
은닉층 결과를 받아 출력하는 출력층


은닉층이 0개인 신경망을 단층신경망
1개인 신경망을 다층 신경망
2개 이상인 신경망을 심층 신경망 이라 한다.

신경망과 퍼셉트론의 차이점?
편향과 활성함수의 구조 !


시그모이드 함수는 절대 사용하지말자
이유 :
    값이 매우 크거나, 작은 경우 함수 값이 포화되기 쉽다.
    함수 값이 zero-centered가 아니기에, 입력값이 항상 양인 경우 가중치에 대한 함수 미분값이 항상 양 또는 음이다.
    지수함수 계산하는데 낭비가 많다.



결론 :
    ReLU 함수를 사용을 권한다.

    함수값의 포화 문제가 없다.
    계산이 빠르다.
    수렴 속도가 약 6배 빠름
단점 :
    입력값이 0보다 작은 경우 함수 미분값이 0이 되는 단점

ReLU는 x값이 커질수록 출력값도 커지므로 not saturate이다.
'''




'''
ReLU의 대안

1. PReLU 함수

    ReLU 함수의 장점을 그대로 가지고 있다.
    ReLU 함수의 입력값이 0보다 작을 때의 약점을 보완했다.
    exp함수를 계산해야 하므로 시그모이드 단점과 비슷한 문제가 있다.
    
    
>> 그저 dying ReLU 현상을 해결하기 위해 제시된 함수이다. 
작은 기울기를 부여한다.


2. ELU (Exponential Linear Unit) 함수
PReLU 함수와 장단점이 같으나 , ReLU의 임계값을 -1로 낮춘 함수를 exp로 이용하여 근사한 것이다.
ReLU의 장점을 모두 포함하고, dying ReLU 해결했으며, 출력값이 거의 제로센터에 가깝다.
하지만 exp 계산해야하는 비용이 든다.

즉 제일 좋은건

공부하는 과정에선 sigmoid와 ReLU함수 사용해서 공부하고
실제 신경망 설계시에는 ReLU와 PReLU함수를 사용하자.


'''
