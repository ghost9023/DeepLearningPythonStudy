#2) 활성함수


#######KEYWORD############
#활성함수 : 개별 가중치와 입력값의 곱의 합에 편향을 더한 값 -> h(x) = wx+b




#########################



#퍼셉트론에서 배웠듯이 활성함수 혹은 활성화함수는 기본적으로 개별 가중치와 입력값의 곱의 합에 편향을 더한 값을 의미.
#퍼셉트론에서 사용한 것과 같은 활성함수를 h(x)라고 할 때
#h(x)=0 or h(x)=1 처럼 입력 값에 따라 0 혹은 1로 단순하게 딱딱 나뉘는 함수를 계단 함수라고 한다.

#단순 계단 함수
def step_function(x):
    if x>0:
        return 1
    else:
        return 0

#numpy array(numpy 배열을 입력으로 받을 수 있는 계단 함수)
import numpy as np
def step_function2(x):
    y = x > 0
    return y.astype(np.int) #numpy array를 변환 할 땐 astype() 메소드를 사용.

#이 계단 함수는 간단하고 단순해서 복잡한 데이터를 입력하는 신경망에 사용할 경우 성능이 좋지 않다.
#따라서 신경망엔 여러가지 복잡한 활성함수를 선택해서 사용하는데

#하이퍼볼릭 탄젠트 함수 (tanh)
#시그모이드 함수
#ReLU 함수

#등을 사용한다.

#현재 딥러닝에서 보편적으로 사용하고 있는 함수는 시그모이드 함수와 ReLU함수이며 이중에서도 최신 트렌드는
#ReLU 함수를 더 많이 사용한다. 이 함수가 타 활성 함수보다 학습 속도와 학습 성능, 효율에서 더 뛰어나기 때문.
#활성함수는 신경망에서 각각의 노드(누런)는 전달 받은 데이터를 가중치를 고려해 합산한 값에 활성함수를 적용해서 다음 층에
#전달한 다음 이 과정을 반복하여 출력층을 통해 결과가 출력되는 알고리즘 구조를 가진다.
#신경망에 작은 값이 입력되었을 때는 활성함수는 출력값을 작은 값으로 막고 일정 값을 초과하면 출력값이 급격히 커지도록 설계됨.
#마치 생물의 뉴런같이.

#
#
#  입력 -> 입력층 -> 활성함수 -> 다음층 -> 활성함수 ... -> 출력층
#
#

