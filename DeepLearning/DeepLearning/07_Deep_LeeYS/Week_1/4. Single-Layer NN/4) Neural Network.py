#4) 신경망 구현하기

##########KEYWORD###############





################################


#신경망은 입력층에서 출력층으로 기본적으로 한 방향으로 흐른다. 한 싸이클이 끝나면 역전파 알고리즘을 통해
#계속 학습으 진행하지만 역전파 알고리즘과 같은 고급 알고리즘은 다음장에서..
#한 방향으로만 정보가 전방으로 전달되는 신경망을 피드포워드 신경망(Feed Forward NN)이라고 한다.
#기본적으로 신경망은 입력층에서 데이터 입력을 받은 뒤 은닉층에서 데이터를 학습하고 출력층으로 결과를 내보낸다.

#입력층의 역할은 입력 데이터를 받아들이는 것이고 이를 위해서 입력층의 노드(뉴런) 개수는 입력데이터의 특성 갯수와 일치해야 한다.
#은닉층은 학습을 진행하는 층으로 은닉층의 노드 수와 은닉층 Layer 수는 설계자가 경험으로 얻어내야 한다.
#뉴런의 수가 너무 많으면 오버피팅이 발생하고 너무 적으면 언더피팅이 발생하여 학습이 되지 않음.
#또한 은닉층의 개수가 지나치게 많은 경우 비효율적이다.
#단순히 은닉층의 개수를 2배 늘리면 연산에 걸리는 시간은 400% 증가하지만 학습효율은 10%만 증가하기도 한다.
#출력층은 은닉층을 거쳐서 얻어낸 결과를 해결하고자 하는 문제에 맞게 만들어 준다.
#필기체 숫자 0부터 9까지를 인식하는 신경망이면 출력층이 10개가 될 것이고 개와 고양이를 분류하는 신경망이라면 3개의 출력층이 된다.

#다차원 배열을 이용하여 층이 3개인 다층 신경망을 간단하게 구현하자.행렬곱과 각 행렬의 원소의 위치를 잘 확인하면 어렵지 않다.
#그림25 P35

#
#Layer        Node 수        Node Shape            Weight Shape              Bias Shape            계산식
#입력층         2            2차원 벡터             2 X 3 Matrix             3차원 Vector     은닉층(1) = 활성화함수((입력층*가중치1) + 편향1)
#은닉층(1)      3            3차원 벡터             3 X 2 Matrix             2차원 Vector     은닉층(2) = 활성화함수((은닉층1) * 가중치2 + 편향2)
#은닉층(2)      2            2차원 벡터             2 X 2 Matrix             2차원 Vector     출력층    = 활성화함수((은닉층2) * 가중치3 + 편향3)
#출력층         2            2차원 벡터

#그림을 확인해보면 3층 신경망이 어떻게 구성되어 있는지 확인할 수 있다.
#입력층은 2개이며 각 층마다 편향이 존재한다. 은닉층은 2개 층으로 구성되어 있고 출력층의 출력값은 2개이다.

#위 그림을 확인해보면

#w12^(1), a2(1) 와 같은 형식으로 표기되어 있는 것을 확인 할 수 있다. 우측 상단의 (1)은 1층의 가중치를 의미한다.
#우측 하단의 12에서 1은 다음층의 뉴런번호 2는 앞층의 뉴런 번호를 의미한다. 따라서 w12^(!)은 앞의 1번 뉴런에서 2번 뉴런으로 이동하는 신경망 1층의 가중치
#를 의미한다.

#예제 3층 신경망의 구조를 보면 입력층은 2개로 구성되어 있고 1층에서 편향이 1로 존재한다. 여기서 가중치에 의해
#입력 값은 a1(1) ... 에 입력된다. 이 입력값을 수식으로 나타내면

#a1(1) = w11^(1)x1 + w12^(1)x2 + b1^(1) 으로 표현할 수 있다.
#이를 행렬 내적으로 표현하면 1층의 노드를 A^(1) = (a1^(1),a2^(1),a3^(1)), 1층의 가중치를
#W^(1) ...

#이를 이용해서 numpy의 다차원 배열을 이용하면 신경망 1층을 파이선 코드로 짤 수 있다.
#마찬가지로 1층의 출력값을 다시 2층의 입력값으로 넣고 똑같은 방식으로 입력노드 행렬(1층의 출력노드 행렬), 가중치 행렬, 편향 행렬의
#행렬 연산을 통해 2층의 출력 노드 행렬을 구할 수 있게 된다.

#마찬가지로 신경망 1층에서 행렬 연산식을 통해 출력값을 구했던 것처럼 1층의 출력값을 2층의 입력값으로 연결해주고 2층의 가중치와 2층의 편향을
#더해주면 2층의 출력값이 완성된다.

#마지막으로 그림30 처럼 2층의 출력값을 동일한 방법으로 출력층의 입력값으로 넣고  출력층 사이의 가중치와 편향을 더해준 동일한 방법으로
#식을 계산하면 최정적인 출력값이 뽑히게 된다. 한가지 위 과정과 다른 점이 있다면 출력층의 활성함수는 풀고자하는 문제의 성질에 맞게 정한다.
#회귀가 목적인 신경망은 출력층에 항등함수를 사용하고 이중클래스 분류에는 시그모이드 함수를 다중 클래스에는 소프트맥스 함수를 일반적으로 사용.
#그럼 출력층에 사용하는 활성함수를 알아보자.
#회귀에는 항등함수, 분류에는 소프트맥스 함수를 보통 사용한다. 회귀는 입력데이터의 연속적인 수치를 예측하는 것을 의미하고 분류는 각 데이터가 어떤
#범주에 속하는지 나누는 것을 의미한다. 항등함수는 입력값이 그대로 출력되는 함수로 흔히 알고 있는 f(x) = x 를 의미한다.
#파이선 코드로는
def identity_function(x):
    return x

#소프트맥스 함수는 자연상수를 밑수로 하는 지수함수로 이루어진 하나의 함수이다.
#소프트맥스 함수가 가지는 의미는 바로 시그모이드 함수를 일반화 한 것.
#이를 통해 각 클래스에 대한 확률을 계산 할 수도 있게 됨.
#시그모이드 함수를 일반화해서 각 클래스에 대한 확률을 계산 할 수 있다는 것은 모든 소프트맥스 함수의 출력값을 더하면 1이 나온다는 의미이다.
#소프트맥스 함수 출력값은 0과 1사이의 값이고 각각의 출력 값은 개별 출력 값에 대한 확률 값이기 때문에 전체 소프트맥스 함수의 합은 항상
#1이 되는 특별한 성질을 가진다.
#때문에 소프트 맥스 함수를 출력층의 활성함수로 사용하면 출력결과를 확률적으로 결론낼 수 있다.
#예를 들어

#y[0] = 0.018, y[1] = 0.245, y[2] = 0.737로 결과가 출력되었다면 1.8%의 확률로 0번 클래스, 24.5%의 확률로 1번 클래스, 73.7%의 확률로 2번
#클래스일 것이므로 2번 클래스일 확률이 가장 높고 따라서 답은 2번 클래스다. 라는 결과를 도출 할 수 있다.
#소프트맥스 함수를 이용해서 통계적(확률적)으로 문제를 대응할 수 있게 되는 것이다. \
#소프트맥스 함수는 단조 증가 함수인 지수함수 exp()를 기반으로 하므로 소프트맥스 함수의 출력값의 대소관계가 그대로 입력된 원소의 대소관계를 이어받는다.
#따라서 역으로 소프트맥스 함수를 통해 나온 출력값의 대소관계를 입력값의 대소관계로 판단해도 된다.
#그래서 신경망 학습과정에선 출력층의 활성함수로 소프트맥스 함수를 사용하고 학습된 모델을 이용해서 추론(분류 및 회귀)하는 과정에선 소프트맥스 함수를
#활성함수에서 생략해도 된다. 이러한 소프트맥스 함수의 구현엔 주의사항이 있다.
#지수함수는 입력값이 커지면 급격하게 무한히 증가한다. 이를 오버플로우(Overflow)라고 한다.
#입력값이 100인 exp(100)은 10의 40승이 넘는 수이다. 오버플로를 해결하기 위해선 해당 값을 전체 데이터 셋에서의 최대값으로 뺀 값으로 치환하는 방법을 사용한다.
#위 과정을 수식으로 나타낼 수 있다. [수식 13] P40

#소프트맥스 함수의 분모 분자에 C라는 상수를 곱해준다. 같은 상수값을 곱해주었으므로 전체 값엔 변화가 없다.
#그리고 여기에 지수함수와 로그함수의 성질 중 하나인 x = a ^ log(a,x)를 이용하여 상수 C를 exp() 함수 안으로 넣는다.
#그럼 상수 C는 exp() 함수 내에서 log(e,C) = ln C 로 변화되고 ln C를 상수 C` 로 받게 되면 아래의 수식으로 변형된다.

#파이선 코드
import numpy as np
a = np.array([1010,1000,990])
np.exp(a) / np.sum(np.exp(a)) #오버플로 발생

#변경된 softmax 함수식
c = np.max(a)
np.exp(a-c) / np.sum(np.exp(a-c)) #정상적으로 계산됨

#이처럼 같은 스케일의 변화는 아무런 결과값에 아무런 영향을 주지 않는 점을 이용해서 소프트맥스 함수의 오버플로 현상을 해결할 수 있다.
#이를 이용하여 소프트맥스 함수를 파이썬으로 구현하면 아래와 같다.
def softmax(a):
    c=np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

#마지막으로 출력층의 노드 개수를 정하는 방법은 간단하다. 입력한 데이터의 클래스 갯수만큼 출력층의 노드 갯수를 정해주면 된다.
#다른 예로 개와 고양이를 분류하고 싶다면 개, 고양이 총 2개의 출력 노드를 만들면 된다.
#은닉층이 2개인 다층 신경망(보통 입력층을 제외한 층수로 신경망을 부른다. 따라서 이 경우는 3층 신경망)
#을 간단하게 파이선으로 코딩.
#이 신경망 모델은 출력층의 활성 함수로 항등함수로 정의한다.
#결과적으로 위 과정을 모두 합한 전체적인 은닉층이 2층인 다층 신경망의 파이썬 구현코드는 아래와 같다.
import numpy as np

#시그모이드 함수
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
#identify function 항등함수 사용
def identify_function(x):
    return x

#신경망을 초기화. 여기서 가중치와 편향의 다차원 배열을 선언해준다.
def init_network():
    network = {}
    network['w1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
    network['b1'] = np.array([0.1,0.2,0.3])
    network['w2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['w3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1,0.2])
    return network

#순전파 신경망 함수. 가중치와 편향을 입력받아 입력층과 은닉층의 활성함수는 시그모이드 함수로,
#출력층의 활성함수는 항등함수를 사용하는 3층 신경망을 함수로 구현
def forward(network,x):
    w1,w2,w3 = network['w1'],network['w2'],network['w3']
    b1,b2,b3 = network['b1'],network['bw'],network['b3']

    a1 = np.dot(x,w1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,w2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2,w3) + b3
    y = identify_function(a3)
    return y

network = init_network() #신경망의 가중치와 편향값 인스턴스화
x = np.array([1.0,0.5])
y = forward(network ,x)
print(y)

#단순한 신경망을 설계하는 것은 어렵지 않다. 다차원 배열을 잘 사용해서 가중치와 입력값 그리고 편향을 잘 도출해서 어떤 활성함수를 사용할지 정해서
#구현한 다음 구현한 활성함수에 값을 잘 넣어준 다음 이전 층의 출력값으로 잘 연결해서 원하는 층만큼 이어주면 된다.
